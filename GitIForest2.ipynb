{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gC7YR4ltm0JW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Isolation Forest Classifier with Differential Privacy\n",
        "\"\"\"\n",
        "from collections import namedtuple\n",
        "import warnings\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import numpy as np\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "from sklearn.tree._tree import Tree, DOUBLE, DTYPE, NODE_DTYPE  # pylint: disable=no-name-in-module\n",
        "from sklearn.tree import DecisionTreeClassifier as skDecisionTreeClassifier\n",
        "\n",
        "from sklearn.ensemble import IsolationForest as skIsolationForest\n",
        "from sklearn.ensemble._forest import _parallel_build_trees #new\n",
        "\n",
        "\n",
        "from diffprivlib.accountant import BudgetAccountant\n",
        "from diffprivlib.utils import PrivacyLeakWarning, check_random_state\n",
        "from diffprivlib.mechanisms import PermuteAndFlip\n",
        "from diffprivlib.validation import DiffprivlibMixin\n",
        "\n",
        "MAX_INT = np.iinfo(np.int32).max\n",
        "\n",
        "import numbers\n",
        "import numpy as np\n",
        "from scipy.sparse import issparse\n",
        "from warnings import warn\n",
        "from numbers import Integral, Real\n",
        "\n",
        "from sklearn.tree import ExtraTreeRegressor\n",
        "from sklearn.tree._tree import DTYPE as tree_dtype\n",
        "from sklearn.utils import (\n",
        "    check_random_state,\n",
        "    check_array,\n",
        "    gen_batches,\n",
        "    get_chunk_n_rows,\n",
        ")\n",
        "from sklearn.utils._param_validation import Interval, StrOptions\n",
        "from sklearn.utils.validation import check_is_fitted, _num_samples\n",
        "from sklearn.base import OutlierMixin\n",
        "\n",
        "from sklearn.ensemble._bagging import BaseBagging\n",
        "\n",
        "__all__ = [\"IsolationForest\"]\n",
        "\n",
        "\n",
        "class IsolationForest(skIsolationForest, DiffprivlibMixin):  # pylint: disable=too-many-ancestors\n",
        "    r\"\"\"\n",
        "    Isolation Forest Algorithm with differential privacy.\n",
        "    This class implements Differentially Private Isolation Forests \n",
        "\n",
        "    Return the anomaly score of each sample using the IsolationForest algorithm\n",
        "    The IsolationForest 'isolates' observations by randomly selecting a feature\n",
        "    and then randomly selecting a split value between the maximum and minimum\n",
        "    values of the selected feature.\n",
        "    Since recursive partitioning can be represented by a tree structure, the\n",
        "    number of splittings required to isolate a sample is equivalent to the path\n",
        "    length from the root node to the terminating node.\n",
        "    This path length, averaged over a forest of such random trees, is a\n",
        "    measure of normality and our decision function.\n",
        "    Random partitioning produces noticeably shorter paths for anomalies.\n",
        "    Hence, when a forest of random trees collectively produce shorter path\n",
        "    lengths for particular samples, they are highly likely to be anomalies.\n",
        "    Read more in the :ref:`User Guide <isolation_forest>`.\n",
        "    .. versionadded:: 0.18\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int, default=100\n",
        "        The number of base estimators in the ensemble.\n",
        "    max_samples : \"auto\", int or float, default=\"auto\"\n",
        "        The number of samples to draw from X to train each base estimator.\n",
        "            - If int, then draw `max_samples` samples.\n",
        "            - If float, then draw `max_samples * X.shape[0]` samples.\n",
        "            - If \"auto\", then `max_samples=min(256, n_samples)`.\n",
        "        If max_samples is larger than the number of samples provided,\n",
        "        all samples will be used for all trees (no sampling).\n",
        "    contamination : 'auto' or float, default='auto'\n",
        "        The amount of contamination of the data set, i.e. the proportion\n",
        "        of outliers in the data set. Used when fitting to define the threshold\n",
        "        on the scores of the samples.\n",
        "            - If 'auto', the threshold is determined as in the\n",
        "              original paper.\n",
        "            - If float, the contamination should be in the range (0, 0.5].\n",
        "        .. versionchanged:: 0.22\n",
        "           The default value of ``contamination`` changed from 0.1\n",
        "           to ``'auto'``.\n",
        "    max_features : int or float, default=1.0\n",
        "        The number of features to draw from X to train each base estimator.\n",
        "            - If int, then draw `max_features` features.\n",
        "            - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n",
        "        Note: using a float number less than 1.0 or integer less than number of\n",
        "        features will enable feature subsampling and leads to a longerr runtime.\n",
        "    bootstrap : bool, default=False\n",
        "        If True, individual trees are fit on random subsets of the training\n",
        "        data sampled with replacement. If False, sampling without replacement\n",
        "        is performed.\n",
        "    n_jobs : int, default=None\n",
        "        The number of jobs to run in parallel for both :meth:`fit` and\n",
        "        :meth:`predict`. ``None`` means 1 unless in a\n",
        "        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
        "        processors. See :term:`Glossary <n_jobs>` for more details.\n",
        "    random_state : int, RandomState instance or None, default=None\n",
        "        Controls the pseudo-randomness of the selection of the feature\n",
        "        and split values for each branching step and each tree in the forest.\n",
        "        Pass an int for reproducible results across multiple function calls.\n",
        "        See :term:`Glossary <random_state>`.\n",
        "    verbose : int, default=0\n",
        "        Controls the verbosity of the tree building process.\n",
        "    warm_start : bool, default=False\n",
        "        When set to ``True``, reuse the solution of the previous call to fit\n",
        "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
        "        new forest. See :term:`the Glossary <warm_start>`.\n",
        "        .. versionadded:: 0.21\n",
        "    Attributes\n",
        "    ----------\n",
        "    estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n",
        "        The child estimator template used to create the collection of\n",
        "        fitted sub-estimators.\n",
        "        .. versionadded:: 1.2\n",
        "           `base_estimator_` was renamed to `estimator_`.\n",
        "    base_estimator_ : ExtraTreeRegressor instance\n",
        "        The child estimator template used to create the collection of\n",
        "        fitted sub-estimators.\n",
        "        .. deprecated:: 1.2\n",
        "            `base_estimator_` is deprecated and will be removed in 1.4.\n",
        "            Use `estimator_` instead.\n",
        "    estimators_ : list of ExtraTreeRegressor instances\n",
        "        The collection of fitted sub-estimators.\n",
        "    estimators_features_ : list of ndarray\n",
        "        The subset of drawn features for each base estimator.\n",
        "    estimators_samples_ : list of ndarray\n",
        "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
        "        estimator.\n",
        "    max_samples_ : int\n",
        "        The actual number of samples.\n",
        "    offset_ : float\n",
        "        Offset used to define the decision function from the raw scores. We\n",
        "        have the relation: ``decision_function = score_samples - offset_``.\n",
        "        ``offset_`` is defined as follows. When the contamination parameter is\n",
        "        set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n",
        "        close to 0 and the scores of outliers are close to -1. When a\n",
        "        contamination parameter different than \"auto\" is provided, the offset\n",
        "        is defined in such a way we obtain the expected number of outliers\n",
        "        (samples with decision function < 0) in training.\n",
        "        .. versionadded:: 0.20\n",
        "    n_features_in_ : int\n",
        "        Number of features seen during :term:`fit`.\n",
        "        .. versionadded:: 0.24\n",
        "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
        "        Names of features seen during :term:`fit`. Defined only when `X`\n",
        "        has feature names that are all strings.\n",
        "        .. versionadded:: 1.0\n",
        "    _parameter_constraints = DiffprivlibMixin._copy_parameter_constraints(\n",
        "        skIsolationForest, \"n_estimators\", \"n_jobs\", \"verbose\", \"random_state\", \"warm_start\")\n",
        "\n",
        "    def __init__(self, n_estimators=100, *, epsilon=1.0, bounds=None, classes=None, n_jobs=None, verbose=0, accountant=None,\n",
        "                 random_state=None, max_depth=5, warm_start=False, shuffle=False, **unused_args):\n",
        "        super().__init__(\n",
        "            n_estimators=n_estimators,\n",
        "            criterion=None,\n",
        "            bootstrap=False,\n",
        "            n_jobs=n_jobs,\n",
        "            random_state=random_state,\n",
        "            verbose=verbose,\n",
        "            warm_start=warm_start)\n",
        "        self.epsilon = epsilon\n",
        "        self.bounds = bounds\n",
        "        self.classes = classes\n",
        "        self.max_depth = max_depth\n",
        "        self.shuffle = shuffle\n",
        "        self.accountant = BudgetAccountant.load_default(accountant)\n",
        "\n",
        "        # Todo: Remove when scikit-learn v1.2 is a min requirement\n",
        "        if hasattr(self, \"estimator\"):\n",
        "            self.estimator = DecisionTreeClassifier()\n",
        "        else:\n",
        "            self.base_estimator = DecisionTreeClassifier()\n",
        "        self.estimator_params = (\"max_depth\", \"epsilon\", \"bounds\", \"classes\")\n",
        "\n",
        "        self._warn_unused_args(unused_args)\"\"\"\n",
        "\n",
        "    def fit(self, X, y=None, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Fit estimator.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            The input samples. Use ``dtype=np.float32`` for maximum\n",
        "            efficiency. Sparse matrices are also supported, use sparse\n",
        "            ``csc_matrix`` for maximum efficiency.\n",
        "        y : Ignored\n",
        "            Not used, present for API consistency by convention.\n",
        "        sample_weight : array-like of shape (n_samples,), default=None\n",
        "            Sample weights. If None, then samples are equally weighted.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Fitted estimator.\n",
        "        \"\"\"\n",
        "        self._validate_params()\n",
        "        #self.accountant.check(self.epsilon, 0)\n",
        "\n",
        "        X = self._validate_data(X, accept_sparse=[\"csc\"], dtype=tree_dtype)\n",
        "        if issparse(X):\n",
        "            # Pre-sort indices to avoid that each individual tree of the\n",
        "            # ensemble sorts the indices.\n",
        "            X.sort_indices()\n",
        "\n",
        "        rnd = check_random_state(self.random_state)\n",
        "        y = rnd.uniform(size=X.shape[0])\n",
        "\n",
        "        # ensure that max_sample is in [1, n_samples]:\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if isinstance(self.max_samples, str) and self.max_samples == \"auto\":\n",
        "            max_samples = min(256, n_samples)\n",
        "\n",
        "        elif isinstance(self.max_samples, numbers.Integral):\n",
        "            if self.max_samples > n_samples:\n",
        "                warn(\n",
        "                    \"max_samples (%s) is greater than the \"\n",
        "                    \"total number of samples (%s). max_samples \"\n",
        "                    \"will be set to n_samples for estimation.\"\n",
        "                    % (self.max_samples, n_samples)\n",
        "                )\n",
        "                max_samples = n_samples\n",
        "            else:\n",
        "                max_samples = self.max_samples\n",
        "        else:  # max_samples is float\n",
        "            max_samples = int(self.max_samples * X.shape[0])\n",
        "\n",
        "        self.max_samples_ = max_samples\n",
        "        max_depth = int(np.ceil(np.log2(max(max_samples, 2))))\n",
        "        super()._fit(\n",
        "            X,\n",
        "            y,\n",
        "            max_samples,\n",
        "            max_depth=max_depth,\n",
        "            sample_weight=sample_weight,\n",
        "            check_input=False,\n",
        "        )\n",
        "\n",
        "        if self.contamination == \"auto\":\n",
        "            # 0.5 plays a special role as described in the original paper.\n",
        "            # we take the opposite as we consider the opposite of their score.\n",
        "            self.offset_ = -0.5\n",
        "            return self\n",
        "\n",
        "        # else, define offset_ wrt contamination parameter\n",
        "        self.offset_ = np.percentile(self.score_samples(X), 100.0 * self.contamination)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict if a particular sample is an outlier or not.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            The input samples. Internally, it will be converted to\n",
        "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
        "            to a sparse ``csr_matrix``.\n",
        "        Returns\n",
        "        -------\n",
        "        is_inlier : ndarray of shape (n_samples,)\n",
        "            For each observation, tells whether or not (+1 or -1) it should\n",
        "            be considered as an inlier according to the fitted model.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self)\n",
        "        decision_func = self.decision_function(X)\n",
        "        is_inlier = np.ones_like(decision_func, dtype=int)\n",
        "        is_inlier[decision_func < 0] = -1\n",
        "        return is_inlier\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        Average anomaly score of X of the base classifiers.\n",
        "        The anomaly score of an input sample is computed as\n",
        "        the mean anomaly score of the trees in the forest.\n",
        "        The measure of normality of an observation given a tree is the depth\n",
        "        of the leaf containing this observation, which is equivalent to\n",
        "        the number of splittings required to isolate this point. In case of\n",
        "        several observations n_left in the leaf, the average path length of\n",
        "        a n_left samples isolation tree is added.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            The input samples. Internally, it will be converted to\n",
        "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
        "            to a sparse ``csr_matrix``.\n",
        "        Returns\n",
        "        -------\n",
        "        scores : ndarray of shape (n_samples,)\n",
        "            The anomaly score of the input samples.\n",
        "            The lower, the more abnormal. Negative scores represent outliers,\n",
        "            positive scores represent inliers.\n",
        "        \"\"\"\n",
        "        # We subtract self.offset_ to make 0 be the threshold value for being\n",
        "        # an outlier:\n",
        "\n",
        "        return self.score_samples(X) - self.offset_\n",
        "\n",
        "    def score_samples(self, X):\n",
        "        \"\"\"\n",
        "        Opposite of the anomaly score defined in the original paper.\n",
        "        The anomaly score of an input sample is computed as\n",
        "        the mean anomaly score of the trees in the forest.\n",
        "        The measure of normality of an observation given a tree is the depth\n",
        "        of the leaf containing this observation, which is equivalent to\n",
        "        the number of splittings required to isolate this point. In case of\n",
        "        several observations n_left in the leaf, the average path length of\n",
        "        a n_left samples isolation tree is added.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            The input samples.\n",
        "        Returns\n",
        "        -------\n",
        "        scores : ndarray of shape (n_samples,)\n",
        "            The anomaly score of the input samples.\n",
        "            The lower, the more abnormal.\n",
        "        \"\"\"\n",
        "        # code structure from ForestClassifier/predict_proba\n",
        "\n",
        "        check_is_fitted(self)\n",
        "\n",
        "        # Check data\n",
        "        X = self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
        "\n",
        "        # Take the opposite of the scores as bigger is better (here less\n",
        "        # abnormal)\n",
        "        return -self._compute_chunked_score_samples(X)\n",
        "\n",
        "    def _compute_chunked_score_samples(self, X):\n",
        "\n",
        "        n_samples = _num_samples(X)\n",
        "\n",
        "        if self._max_features == X.shape[1]:\n",
        "            subsample_features = False\n",
        "        else:\n",
        "            subsample_features = True\n",
        "\n",
        "        # We get as many rows as possible within our working_memory budget\n",
        "        # (defined by sklearn.get_config()['working_memory']) to store\n",
        "        # self._max_features in each row during computation.\n",
        "        #\n",
        "        # Note:\n",
        "        #  - this will get at least 1 row, even if 1 row of score will\n",
        "        #    exceed working_memory.\n",
        "        #  - this does only account for temporary memory usage while loading\n",
        "        #    the data needed to compute the scores -- the returned scores\n",
        "        #    themselves are 1D.\n",
        "\n",
        "        chunk_n_rows = get_chunk_n_rows(\n",
        "            row_bytes=16 * self._max_features, max_n_rows=n_samples\n",
        "        )\n",
        "        slices = gen_batches(n_samples, chunk_n_rows)\n",
        "\n",
        "        scores = np.zeros(n_samples, order=\"f\")\n",
        "\n",
        "        for sl in slices:\n",
        "            # compute score on the slices of test samples:\n",
        "            scores[sl] = self._compute_score_samples(X[sl], subsample_features)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def _compute_score_samples(self, X, subsample_features):\n",
        "        \"\"\"\n",
        "        Compute the score of each samples in X going through the extra trees.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like or sparse matrix\n",
        "            Data matrix.\n",
        "        subsample_features : bool\n",
        "            Whether features should be subsampled.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        depths = np.zeros(n_samples, order=\"f\")\n",
        "\n",
        "        for tree, features in zip(self.estimators_, self.estimators_features_):\n",
        "            X_subset = X[:, features] if subsample_features else X\n",
        "\n",
        "            leaves_index = tree.apply(X_subset)\n",
        "            node_indicator = tree.decision_path(X_subset)\n",
        "            n_samples_leaf = tree.tree_.n_node_samples[leaves_index]\n",
        "\n",
        "            depths += (\n",
        "                np.ravel(node_indicator.sum(axis=1))\n",
        "                + _average_path_length(n_samples_leaf)\n",
        "                - 1.0\n",
        "            )\n",
        "        denominator = len(self.estimators_) * _average_path_length([self.max_samples_])\n",
        "        scores = 2 ** (\n",
        "            # For a single training sample, denominator and depth are 0.\n",
        "            # Therefore, we set the score manually to 1.\n",
        "            -np.divide(\n",
        "                depths, denominator, out=np.ones_like(depths), where=denominator != 0\n",
        "            )\n",
        "        )\n",
        "        return scores\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {\n",
        "            \"_xfail_checks\": {\n",
        "                \"check_sample_weights_invariance\": (\n",
        "                    \"zero sample_weight is not equivalent to removing samples\"\n",
        "                ),\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "def _average_path_length(n_samples_leaf):\n",
        "    \"\"\"\n",
        "    The average path length in a n_samples iTree, which is equal to\n",
        "    the average path length of an unsuccessful BST search since the\n",
        "    latter has the same structure as an isolation tree.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples_leaf : array-like of shape (n_samples,)\n",
        "        The number of training samples in each test sample leaf, for\n",
        "        each estimators.\n",
        "    Returns\n",
        "    -------\n",
        "    average_path_length : ndarray of shape (n_samples,)\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples_leaf = check_array(n_samples_leaf, ensure_2d=False)\n",
        "\n",
        "    n_samples_leaf_shape = n_samples_leaf.shape\n",
        "    n_samples_leaf = n_samples_leaf.reshape((1, -1))\n",
        "    average_path_length = np.zeros(n_samples_leaf.shape)\n",
        "\n",
        "    mask_1 = n_samples_leaf <= 1\n",
        "    mask_2 = n_samples_leaf == 2\n",
        "    not_mask = ~np.logical_or(mask_1, mask_2)\n",
        "\n",
        "    average_path_length[mask_1] = 0.0\n",
        "    average_path_length[mask_2] = 1.0\n",
        "    average_path_length[not_mask] = (\n",
        "        2.0 * (np.log(n_samples_leaf[not_mask] - 1.0) + np.euler_gamma)\n",
        "        - 2.0 * (n_samples_leaf[not_mask] - 1.0) / n_samples_leaf[not_mask]\n",
        "    )\n",
        "\n",
        "    return average_path_length.reshape(n_samples_leaf_shape)\n",
        "      \n",
        "\n",
        "\n",
        "class DecisionTreeClassifier(skDecisionTreeClassifier, DiffprivlibMixin):\n",
        "    r\"\"\"Decision Tree Classifier with differential privacy.\n",
        "    This class implements the base differentially private decision tree classifier\n",
        "    for the Isolation Forest classifier algorithm. Not meant to be used separately.\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int, default: 5\n",
        "        The maximum depth of the tree.\n",
        "    epsilon : float, default: 1.0\n",
        "        Privacy parameter :math:`\\epsilon`.\n",
        "    bounds : tuple, optional\n",
        "        Bounds of the data, provided as a tuple of the form (min, max).  `min` and `max` can either be scalars, covering\n",
        "        the min/max of the entire data, or vectors with one entry per feature.  If not provided, the bounds are computed\n",
        "        on the data when ``.fit()`` is first called, resulting in a :class:`.PrivacyLeakWarning`.\n",
        "    classes : array-like of shape (n_classes,), optional\n",
        "        Array of class labels. If not provided, the classes will be read from the data when ``.fit()`` is first called,\n",
        "        resulting in a :class:`.PrivacyLeakWarning`.\n",
        "    random_state : int or RandomState, optional\n",
        "        Controls the randomness of the estimator.  At each split, the feature to split on is chosen randomly, as is the\n",
        "        threshold at which to split.  The classification label at each leaf is then randomised, subject to differential\n",
        "        privacy constraints. To obtain a deterministic behaviour during randomisation, ``random_state`` has to be fixed\n",
        "        to an integer.\n",
        "    accountant : BudgetAccountant, optional\n",
        "        Accountant to keep track of privacy budget.\n",
        "    Attributes\n",
        "    ----------\n",
        "    n_features_in_: int\n",
        "        The number of features when fit is performed.\n",
        "    n_classes_: int\n",
        "        The number of classes.\n",
        "    classes_: array of shape (n_classes, )\n",
        "        The class labels.\n",
        "    \"\"\"\n",
        "\n",
        "    _parameter_constraints = DiffprivlibMixin._copy_parameter_constraints(\n",
        "        skDecisionTreeClassifier, \"max_depth\", \"random_state\")\n",
        "\n",
        "    def __init__(self, max_depth=5, *, epsilon=1, bounds=None, classes=None, random_state=None, accountant=None,\n",
        "                 **unused_args):\n",
        "        # Todo: Remove when scikit-learn v1.0 is a min requirement\n",
        "        try:\n",
        "            super().__init__(  # pylint: disable=unexpected-keyword-arg\n",
        "                criterion=None,\n",
        "                splitter=None,\n",
        "                max_depth=max_depth,\n",
        "                min_samples_split=None,\n",
        "                min_samples_leaf=None,\n",
        "                min_weight_fraction_leaf=None,\n",
        "                max_features=None,\n",
        "                random_state=random_state,\n",
        "                max_leaf_nodes=None,\n",
        "                min_impurity_decrease=None,\n",
        "                min_impurity_split=None\n",
        "            )\n",
        "        except TypeError:\n",
        "            super().__init__(\n",
        "                criterion=None,\n",
        "                splitter=None,\n",
        "                max_depth=max_depth,\n",
        "                min_samples_split=None,\n",
        "                min_samples_leaf=None,\n",
        "                min_weight_fraction_leaf=None,\n",
        "                max_features=None,\n",
        "                random_state=random_state,\n",
        "                max_leaf_nodes=None,\n",
        "                min_impurity_decrease=None\n",
        "            )\n",
        "        self.epsilon = epsilon\n",
        "        self.bounds = bounds\n",
        "        self.classes = classes\n",
        "        self.accountant = BudgetAccountant.load_default(accountant)\n",
        "\n",
        "        self._warn_unused_args(unused_args)\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None, check_input=True):\n",
        "        \"\"\"Build a differentially-private decision tree classifier from the training set (X, y).\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            The training input samples. Internally, it will be converted to ``dtype=np.float32``.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            The target values (class labels) as integers or strings.\n",
        "        sample_weight : ignored\n",
        "            Ignored by diffprivlib.  Present for consistency with sklearn API.\n",
        "        check_input : bool, default=True\n",
        "            Allow to bypass several input checking. Don't use this parameter unless you know what you do.\n",
        "        Returns\n",
        "        -------\n",
        "        self : DecisionTreeClassifier\n",
        "            Fitted estimator.\n",
        "        \"\"\"\n",
        "        self._validate_params()\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        self.accountant.check(self.epsilon, 0)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            self._warn_unused_args(\"sample_weight\")\n",
        "\n",
        "        if check_input:\n",
        "            X, y = self._validate_data(X, y, multi_output=False)\n",
        "        self.n_outputs_ = 1\n",
        "\n",
        "        if self.bounds is None:\n",
        "            warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n",
        "                          \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n",
        "                          \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n",
        "            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n",
        "        self.bounds = self._check_bounds(self.bounds, shape=X.shape[1])\n",
        "        X = self._clip_to_bounds(X, self.bounds)\n",
        "\n",
        "        if self.classes is None:\n",
        "            warnings.warn(\"Classes have not been specified and will be calculated on the data provided. This will \"\n",
        "                          \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n",
        "                          \"privacy leakage, specify the prediction classes for model.\", PrivacyLeakWarning)\n",
        "            self.classes = np.unique(y)\n",
        "        self.classes_ = np.ravel(self.classes)\n",
        "        self.n_classes_ = len(self.classes_)\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        # Build and fit the _FittingTree\n",
        "        fitting_tree = _FittingTree(self.max_depth, self.n_features_in_, self.classes_, self.epsilon, self.bounds,\n",
        "                                    random_state)\n",
        "        fitting_tree.build()\n",
        "        fitting_tree.fit(X, y)\n",
        "\n",
        "        # Load params from _FittingTree into sklearn.Tree\n",
        "        d = fitting_tree.__getstate__()\n",
        "        tree = Tree(self.n_features_in_, np.array([self.n_classes_]), self.n_outputs_)\n",
        "        tree.__setstate__(d)\n",
        "        self.tree_ = tree\n",
        "\n",
        "        self.accountant.spend(self.epsilon, 0)\n",
        "\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def n_features_(self):\n",
        "        return self.n_features_in_\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {}\n",
        "\n",
        "\n",
        "class _FittingTree(DiffprivlibMixin):\n",
        "    r\"\"\"Array-based representation of a binary decision tree, trained with differential privacy.\n",
        "    This tree mimics the architecture of the corresponding Tree from sklearn.tree.tree_, but without many methods given\n",
        "    in Tree. The purpose of _FittingTree is to fit the parameters of the model, and have those parameters passed to\n",
        "    Tree (using _FittingTree.__getstate__() and Tree.__setstate__()), to be used for prediction.\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth : int\n",
        "        The maximum depth of the tree.\n",
        "    n_features : int\n",
        "        The number of features of the training dataset.\n",
        "    classes : array-like of shape (n_classes,)\n",
        "        The classes of the training dataset.\n",
        "    epsilon : float\n",
        "        Privacy parameter :math:`\\epsilon`.\n",
        "    bounds : tuple\n",
        "        Bounds of the data, provided as a tuple of the form (min, max).  `min` and `max` can either be scalars, covering\n",
        "        the min/max of the entire data.\n",
        "    random_state : RandomState\n",
        "        Controls the randomness of the building and training process: the feature to split at each node, the threshold\n",
        "        to split at and the randomisation of the label at each leaf.\n",
        "    \"\"\"\n",
        "    _TREE_LEAF = -1\n",
        "    _TREE_UNDEFINED = -2\n",
        "    StackNode = namedtuple(\"StackNode\", [\"parent\", \"is_left\", \"depth\", \"bounds\"])\n",
        "\n",
        "    def __init__(self, max_depth, n_features, classes, epsilon, bounds, random_state):\n",
        "        self.node_count = 0\n",
        "        self.nodes = []\n",
        "        self.max_depth = max_depth\n",
        "        self.n_features = n_features\n",
        "        self.classes = classes\n",
        "        self.epsilon = epsilon\n",
        "        self.bounds = bounds\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def __getstate__(self):\n",
        "        \"\"\"Get state of _FittingTree to feed into __setstate__ of sklearn.Tree\"\"\"\n",
        "        d = {\"max_depth\": self.max_depth,\n",
        "             \"node_count\": self.node_count,\n",
        "             \"nodes\": np.array([tuple(node) for node in self.nodes], dtype=NODE_DTYPE),\n",
        "             \"values\": self.values_}\n",
        "        return d\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"Build the decision tree using random feature selection and random thresholding.\"\"\"\n",
        "        stack = [self.StackNode(parent=self._TREE_UNDEFINED, is_left=False, depth=0, bounds=self.bounds)]\n",
        "\n",
        "        while stack:\n",
        "            parent, is_left, depth, bounds = stack.pop()\n",
        "            node_id = self.node_count\n",
        "            bounds_lower, bounds_upper = self._check_bounds(bounds, shape=self.n_features)\n",
        "\n",
        "            # Update parent node with its child\n",
        "            if parent != self._TREE_UNDEFINED:\n",
        "                if is_left:\n",
        "                    self.nodes[parent].left_child = node_id\n",
        "                else:\n",
        "                    self.nodes[parent].right_child = node_id\n",
        "\n",
        "            # Check if we have a leaf node, then add it\n",
        "            if depth >= self.max_depth:\n",
        "                node = _Node(node_id, self._TREE_UNDEFINED, self._TREE_UNDEFINED)\n",
        "                node.left_child = self._TREE_LEAF\n",
        "                node.right_child = self._TREE_LEAF\n",
        "\n",
        "                self.nodes.append(node)\n",
        "                self.node_count += 1\n",
        "                continue\n",
        "\n",
        "            # We have a decision node, so pick feature and threshold\n",
        "            feature = self.random_state.randint(self.n_features)\n",
        "            threshold = self.random_state.uniform(bounds_lower[feature], bounds_upper[feature])\n",
        "\n",
        "            left_bounds_upper = bounds_upper.copy()\n",
        "            left_bounds_upper[feature] = threshold\n",
        "            right_bounds_lower = bounds_lower.copy()\n",
        "            right_bounds_lower[feature] = threshold\n",
        "\n",
        "            self.nodes.append(_Node(node_id, feature, threshold))\n",
        "            self.node_count += 1\n",
        "\n",
        "            stack.append(self.StackNode(parent=node_id, is_left=True, depth=depth+1,\n",
        "                                        bounds=(bounds_lower, left_bounds_upper)))\n",
        "            stack.append(self.StackNode(parent=node_id, is_left=False, depth=depth+1,\n",
        "                                        bounds=(right_bounds_lower, bounds_upper)))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the tree to the given training data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Target vector relative to X.\n",
        "        \"\"\"\n",
        "        if not self.nodes:\n",
        "            raise ValueError(\"Fitting Tree must be built before calling fit().\")\n",
        "\n",
        "        leaves = self.apply(X)\n",
        "        unique_leaves = np.unique(leaves)\n",
        "        values = np.zeros(shape=(self.node_count, 1, len(self.classes)))\n",
        "\n",
        "        # Populate value of real leaves\n",
        "        for leaf in unique_leaves:\n",
        "            idxs = (leaves == leaf)\n",
        "            leaf_y = y[idxs]\n",
        "\n",
        "            counts = [np.sum(leaf_y == cls) for cls in self.classes]\n",
        "            mech = PermuteAndFlip(epsilon=self.epsilon, sensitivity=1, monotonic=True, utility=counts,\n",
        "                                  random_state=self.random_state)\n",
        "            values[leaf, 0, mech.randomise()] = 1\n",
        "\n",
        "        # Populate value of empty leaves\n",
        "        for node in self.nodes:\n",
        "            if values[node.node_id].sum() or node.left_child != self._TREE_LEAF:\n",
        "                continue\n",
        "\n",
        "            values[node.node_id, 0, self.random_state.randint(len(self.classes))] = 1\n",
        "\n",
        "        self.values_ = values\n",
        "\n",
        "        return self\n",
        "\n",
        "    def apply(self, X):\n",
        "        \"\"\"Finds the terminal region (=leaf node) for each sample in X.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        out = np.zeros((n_samples,), dtype=int)\n",
        "        out_ptr = out.data\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            node = self.nodes[0]\n",
        "\n",
        "            while node.left_child != self._TREE_LEAF:\n",
        "                if X[i, node.feature] <= node.threshold:\n",
        "                    node = self.nodes[node.left_child]\n",
        "                else:\n",
        "                    node = self.nodes[node.right_child]\n",
        "\n",
        "            out_ptr[i] = node.node_id\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class _Node:\n",
        "    \"\"\"Base storage structure for the nodes in a _FittingTree object.\"\"\"\n",
        "    def __init__(self, node_id, feature, threshold):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left_child = -1\n",
        "        self.right_child = -1\n",
        "        self.node_id = node_id\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Defines parameters needed to populate NODE_DTYPE for Tree.__setstate__ using tuple(_Node).\"\"\"\n",
        "        yield self.left_child\n",
        "        yield self.right_child\n",
        "        yield self.feature\n",
        "        yield self.threshold\n",
        "        yield 0.0  # Impurity\n",
        "        yield 0  # n_node_samples\n",
        "        yield 0.0  # weighted_n_node_samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ..tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha5AIRXx1iEd",
        "outputId": "b737fcde-67d1-4731-9204-5ce235d1c793"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '..tree'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install diffprivlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8L6VOX_nkek",
        "outputId": "b91ff420-0a86-4b7b-c590-3bf3c44ff0c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting diffprivlib\n",
            "  Downloading diffprivlib-0.6.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.9/dist-packages (from diffprivlib) (1.10.1)\n",
            "Requirement already satisfied: setuptools>=49.0.0 in /usr/local/lib/python3.9/dist-packages (from diffprivlib) (67.6.1)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.9/dist-packages (from diffprivlib) (1.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.9/dist-packages (from diffprivlib) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from diffprivlib) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.24.2->diffprivlib) (3.1.0)\n",
            "Installing collected packages: diffprivlib\n",
            "Successfully installed diffprivlib-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "obj1= IsolationForest()\n",
        "X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0,\n",
        "                            random_state=0, shuffle=False)\n",
        "clf = obj1.fit(X, y)\n",
        "\n",
        "print(clf.predict([[0, 0, 0, 0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cnq3FR3uRX4",
        "outputId": "990fd966-a0b9-4af5-8c89-0afae50b66f9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4EovMDNquKt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3h7DE1fMuKwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aqlmqWgFuKyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzbwbKTiuK0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smm6l-QauK3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}